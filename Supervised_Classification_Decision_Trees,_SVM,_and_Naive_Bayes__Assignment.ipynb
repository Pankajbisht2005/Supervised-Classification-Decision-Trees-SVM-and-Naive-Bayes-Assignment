{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 : What is Information Gain, and how is it used in Decision Trees?\n",
        "-  Information Gain (IG): It measures the reduction in uncertainty (entropy) after splitting a dataset based on an attribute.\n",
        "\n",
        "\n",
        "- Used in descision trees:\\\n",
        "In Decision Trees, IG helps select the best attribute for splitting at each node. The attribute with the highest IG is chosen, ensuring maximum reduction in impurity.\n",
        "\n"
      ],
      "metadata": {
        "id": "-DXXrcglc-9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "- Definition:\n",
        "\n",
        " **Entropy**:\n",
        "\n",
        "  Measures the amount of disorder or uncertainty in the dataset.\n",
        "\n",
        "**Gini Impurity**:\n",
        "\n",
        " Measures the probability of incorrectly classifying a randomly chosen element.\n",
        "- Computation:\n",
        "\n",
        " Entropy involves logarithms, making it slightly more computationally expensive.\n",
        "\n",
        "Gini is simpler and faster to compute since it only requires squaring probabilities.\n",
        "\n",
        "- Interpretation:\n",
        "\n",
        " Entropy comes from information theory and quantifies the expected information needed to classify a sample.\n",
        "\n",
        " Gini focuses on classification error probability, giving a more direct measure of impurity.\n",
        "\n",
        "- Strengths:\n",
        "\n",
        " Entropy: Theoretically grounded, useful when interpretability in terms of information gain is important.\n",
        "\n",
        "Gini: Computationally efficient, often leads to faster training and slightly purer splits.\n",
        "\n",
        "- Weaknesses:\n",
        "\n",
        "Entropy can be slower due to logarithmic calculations.\n",
        "\n",
        "Gini may sometimes bias splits toward attributes with more categories.\n",
        "- Use Cases:\n",
        "\n",
        " Entropy: Preferred in academic or theoretical contexts where information theory is emphasized.\n",
        "\n",
        "Gini: Commonly used in practice because of speed and simplicity.\n"
      ],
      "metadata": {
        "id": "gnY26U0Yc-60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3:What is Pre-Pruning in Decision Trees?\n",
        "- Definition:\n",
        "\n",
        "Pre-pruning is also called early stopping. It is a technique used in Decision Trees to halt the growth of the tree before it becomes overly complex. Instead of allowing the tree to grow fully and then trimming it , pre-pruning sets constraints during training to prevent overfitting\n"
      ],
      "metadata": {
        "id": "lxxSFy0Bc-4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Question 4:Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).\n",
        "Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\"\"\"\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Create Decision Tree model using Gini Impurity\n",
        "dt_model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "dt_model.fit(X, y)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "print(dt_model.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j06xXjYEf6lt",
        "outputId": "05c541f1-64b8-466c-fb85-2f6b5c5a02ae"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "[0.01333333 0.         0.56405596 0.42261071]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What is a Support Vector Machine (SVM)?\n",
        "- A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression that works by finding an optimal hyperplane which separates data points of different classes with the maximum possible margin."
      ],
      "metadata": {
        "id": "tSSEE7FogXrK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: What is the Kernel Trick in SVM?\n",
        "\n",
        "- The Kernel Trick is a mathematical technique used in Support Vector Machines (SVMs) to handle data that is not linearly separable. Instead of explicitly transforming the data into a higher-dimensional space, the kernel trick computes the similarity between data points in that space using a kernel function.\n"
      ],
      "metadata": {
        "id": "MfbNK2mVgnIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Question 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n",
        "Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting\n",
        "on the same dataset.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\"\"\"\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train SVM with Linear Kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "acc_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Train SVM with RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print accuracies\n",
        "print(\"Linear Kernel Accuracy:\", acc_linear)\n",
        "print(\"RBF Kernel Accuracy:\", acc_rbf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3RG0xqqf6iU",
        "outputId": "46ee9dc2-fa04-4383-eb75-3f766c347197"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 0.9814814814814815\n",
            "RBF Kernel Accuracy: 0.7592592592592593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "-  Definition:\n",
        "\n",
        "The Naïve Bayes classifier is a probabilistic machine learning algorithm based on Bayes’ Theorem. It is widely used for classification tasks, especially in text analysis, spam detection, and sentiment analysis.\n",
        "\n",
        "- Why it is called \"Naïve\":\n",
        "\n",
        "The algorithm assumes all features are independent of each other given the class label.\n",
        "\n",
        "In reality, features often have correlations.\n",
        "\n",
        "Despite this unrealistic assumption, the model performs surprisingly well in practice, hence the name “Naïve.”\n",
        "\n"
      ],
      "metadata": {
        "id": "eL4x-rYQc-2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes.\n",
        "-  Gaussian Naïve Bayes (GNB):\n",
        "\n",
        " Assumes that features follow a normal (Gaussian) distribution.\n",
        "\n",
        "Suitable for continuous data (e.g., height, weight, sensor readings).\n",
        "\n",
        " Example use case: Medical diagnosis where features are continuous measurements.\n",
        "\n",
        "- Multinomial Naïve Bayes (MNB):\n",
        "\n",
        " Assumes that features represent discrete counts or frequencies.\n",
        "\n",
        " Commonly used in text classification (word counts, term frequencies).\n",
        "\n",
        "Example use case: Spam detection using word frequency in emails.\n",
        "\n",
        "- Bernoulli Naïve Bayes (BNB):\n",
        "\n",
        "Assumes that features are binary (0/1), representing presence or absence of a feature.\n",
        "\n",
        " Suitable for data where attributes are yes/no, true/false, or present/absent.\n",
        "\n",
        " Example use case: Document classification based on whether a word appears or not.\n",
        "\n"
      ],
      "metadata": {
        "id": "85NdKFdfc-0n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35Y70kI0c9o0",
        "outputId": "7c6da334-fb50-49f2-ff51-f76470f94392"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9415204678362573\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Question 10: Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy.\n",
        "Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from\n",
        "sklearn.datasets.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\"\"\"\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JDwSDgcIiuXN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}